\chapter{Discussion: Deep Learning for Medical Image Analysis}

In this thesis, we explored new potential applications of deep learning for
medical image analysis. The success of deep learning methods mostly stems from
their ability to learn feature hierarchies directly from the data. This allows
deep learning methods to adapt to the challenges presented by the data, such as
imaging artifacts and contrast variations, without explicitly having to model
them, which makes deep learning methods very flexible and generally applicable
to a wide range of problems. However, deep learning methods can take a lot of
time to train and require large amounts of training data in order to learn a
feature representation that generalizes well to new data samples. To overcome
those challenges, first applications of deep learning were limited to problems
that can be cast as a patch-wise classification problem, such as the
segmentation of cell membranes \citep{ciresan2012} and the detection of cell
carcinoma \citep{cancer}. In this fashion, every patch of an image is considered
a training sample, which greatly increases the amount of labelled training
cases. In addition, training on relatively small 2D patches made training
feasible. Similarly, fully convolutional approach for image segmentation
leverage every pixel of an image as a training sample, while being more
computationally efficient than patch-based approaches. However, layers deeper in
the hierarchy have increasingly fewer pixels, and are consequently trained with
fewer training samples, which increases the risk of overfitting.

The application to entire images using deep learning is still a challenge, due
to the higher computational burden and less amount of available labelled
training data. Unsupervised approaches are a promising alternative to supervised
approaches, because they generally are less prone to overfitting and can
leverage large amounts of unlabelled training data, which is easier to require,
but are still difficult to train due to the large number of trainable
parameters. Alternatively, deep learning based approaches for classifying entire
images rely on the extraction of a large set of manually designed features,
which is in contrast to the end-to-end learning paradigm. In this thesis, we
have proposed a new training algorithms for convolutional models that performs
training in the frequency domain. Our training algorithm made training on entire
images feasible for the first time.

\section{Future Work}

% A possible direction for future work is the development of network architecture
% with build-in invariance properties. Although neural networks can learn feature
% hierarchies that are invariant to rotations and translations, doing so requires
% increasing the size of the network, which increases the number of trainable
% parameters, the time required to train the network and the risk of overfitting.
% To illustrate, consider the example of learning translation invariant features
% using a dense neural network. Features learned by a dense neural network are not
% inherently translational invariant. However, translation invariant features can
% be learned through the combination of two layers. Units of the first layer
% learn the same feature at different locations in the image, while the unit of the
% second layer that corresponds to that features combines the activations of the
% units of the previous layer, such that the unit is active when the feature is
% detected at any position in the image. Similarly, rotation invariant features
% can be learned by learning the same feature at different angles followed by the
% summation of the activation of the different features. However, this greatly
% increases the number of features that need to be learned in order to capture a
% particular feature at a wide range of angels. This problem is elevated when
% learning features in 3D, due to the presence of three rotation parameters
% instead of only one in 2D. It might therefore be important to develop network
% architectures that are inherently invariant or equivariant to rotations,
% similarly to convolutional neural networks in order to reduce the number of
% features that need to be learned in 3D.

One way of dealing with the problem of limited training data is data
augmentation. Data augmentation allows the artificial increase of the training
data size by generating new training samples from existing once, in order to
increase the variability in the data set. Hand-engineered data augmentation
approaches depend on how well the variability in the data is understood and try
to emulate the variability. It has been successfully be used for the recognition
of hand-written digits and other natural image classification problem. Data
augmentation can be used to encourage the learning of a classify that is robust
to the variations added to the data set, such as rotation, translation, contrast
variations and deformations.

Although neural networks are able to learn features that are invariant to the
variability in the data, doing so often requires the learning of different
variants of the same feature in order to capture the range of variability. In
order to decrease the number of weights of a neural network, and therefore to
reduce the training time and risk of overfitting, different neural network
architectures have been developed with build-in invariance or equivariance
properties. For example, previously used sigmoidal activation functions are
sensitive to the intensity of the input, which requires the relearning of
feature detectors for images with varying contrast. To overcome, rectified
linear units were proposed, which represent the same units with different bias,
which makes them equivariant to different contrasts. In order to allow the
learning of translational invariant features, convolutional neural networks
were developed, which greatly reduces the number of features that need to be
learned for natural images compared to dense neural networks. However, features
learned by a convolutional neural network are not invariant to rations. This
leads, for example, to the learning of a variety of edge detecting features that
are sensitive to different angles. While the number of angles is rather low in
2D, significantly more features are required to span the space of rotations in
3D, due to the combination of three rotation parameters instead of only one. In
order to learn sensitive features in 3D, it might therefore be necessary to
develop neural network architectures that are inherently invariant or
equivariant to rotations in order to reduce the number of variants of the same
feature that need to be learned in order to detect the feature under different
angles.

Neural network architectures were inspired by the calculations that can be
performed by the nerve cells of the central nervous system. However, the units
of an artificial neural network can only calculate the the weighted sums of its
inputs followed by a non-linear function, which is an oversimplified model of
the computational properties of the nerve cells in the CNS. In their review of
the computational properties of dendrites, \citet{london2005} noted that some
neurons were found that are able to calculate the multiplication of its inputs.
For example, the looming sensitive neurons in the locust's visual system are
able to detect approaching objects on a collision course by calculating the
multiplication of angular size and speed of approaching object. The
multiplication is possible encoded as the sum of logarithmic inputs followed by
exponentiation. Similarly, \citet{logarithmic} have proposed a type of neuron
for artificial neural networks that is able to learn to calculate the logarithm
or exponential of its inputs, which allows for the learning of networks that can
calculate arbitrary polynomial with significantly fewer parameters than
traditional ReLU or sigmoid neural networks. Alternatively, \citet{dpn} have
proposed a network architecture that can learn arbitrary polynomials using a
combination of units that calculate weighted sums of an arbitrary number of
inputs and units that calculate the product of two inputs. However, these
network architectures needs to be investigated further in order to show of the
theoretical advantages can be translated into improvements on practical
problems.

\begin{comment}

Current state

- strengths of deep learning
- requires large amounts of training data
	- limits problems it can be applied to
	- often only relatively few training samples available
	- patch-based
		- every patch is a training sample
		- segmentation of cell membranes
		- cell detection
	- fully convolutional
		- every pixel is a training sample
		- u-net
		- segmentation
		- localize key points
		- but deeper layers have fewer pixels
		- might overfit in deep layers
	- entire image classification more challenging because less data
		- Full image applications limited due to limited amount of training data and
		  the risk of overfitting
		- entire images when risk of overfitting is lower
			- generative models
			- regularization
			- pattern detection
			- modelling variability
		- use of unlabelled data
			- unlabelled data easier to acquire
			- more unlabelled than labelled data
			- unsupervised feature learning
				- AD classification
		- data augmentation
			- Good understanding of variability
				- hand-engineered data augmentation
				- translations, rotations, deformations, contrast variations
			- Conservative data augmentation, parameter choices
				- Might not reflect the true variability well
			- Crude data augmentation
				- Might not be biologically plausible
			- Learn variability from data
				- Manifold learning and sampling
		- regularization
- can be slow
	- training on patches
	- application to 2D images (cell membrane detection, cancer detection)
	- training in the frequency domain
		- allows training in 3D for the first time
	- fully convolutional approach eliminates redundant calculations
	
Future work

- collect labelled data and make it available to researchers
	- similar to computer vision community
- learning features that are invariant or equivariant under various influences
	- translational in- or equivariance
		- convolutional models
		- reduce relearning of feature at different locations
	- intensity in- or equivariance
		- rectified linear units to some extend
		- avoid relearning of units for difference intensities
	- new: rotational invariance or equivariance
		- reduces the number of features, because only one instance of a feature needs
		  to be learned, instead of multiple for different angles
		- Esp. important in 3D were there are a lot more possible rotation angles
		- Ronneberger et al.
- new applications
- understanding of deep learning
- visualization
- better training algorithms
	- fast convergence
	- saddle point problem
- long distance relationships
	- currently, only way is to increase the size of the receptive field
	- more context
	- However, this also leads to the learning of features at a larger scale
	- need a way to model the correlation between distant small scale features
	- e.g., tissue transition instead of tissue type
	- edge is a small scale feature but affects the interpretation that follows
	  afterwards
	- RNNs and esp. LSTMs can do that
- keep learning from biological neural networks
- what can neurons compute \citep{london2005}
	- long-term or long-distance relationships and memory
		- RNNs, LSTMs
	- multiplications
		- Looming Sensitive Neurons in the Locust
			- fires on objects approaching a collision course
			- multiplication of angular size and speed of approaching
			- possibly encoded as the sum of logarithmic inputs followed
			  by exponentiation
		- e.g., auditory system
		- Three way connection and reasoning
		- Gated RBMs
			- learn transformations
			- better model of natural images
			-> better manifold learning of brain MRIs?
		- multiplications in neural networks
			- unroll gated RBM into neural network
				- if that does not work, maybe generative models are still
				  a good idea
			- deep polynomial networks
			
Challenges: Require large amounts of training data, how to get more labels, and
can be too slow to be feasible? Use every patch of an image as a training
sample. More training data and smaller images. Patch-based segmentation of
membranes one of the first applications that show cased deep learning. Other
applications are cell cancer detection, because again, every cell is a sample
and there are a lot of slides, plus it's done in 2D, so it can be applied
faster. However, patch-based approaches have some weaknesses: 1) can't use every
possible patch, it would be slow, 2) need patch-selection, which can be
difficult to decide. Fully convolutional models can help here. E.g. u-net but
also only applied to 2D images.

In this thesis, we have shown how to speed up the training in order to apply it
to 3D data. First paper to show how deep learning can be made feasible in order
to apply it to full-resolution 3D medical images. Open new possibilities for
research.

A remaining challenge is how to deal with the limited amount of training data.
Limited data currently limits its application to other problems such as the
classification of entire volumes. Need more real or artificial data. If the data
variations of the data is well understood, can perform hand-engineered data
augmentation such as transformations and rotations, deformations. However,
complex data that is not well understood might require learning of the data
manifold.

Tasks deep learning can be good at? Tedious and well defined tasks for which
training data is available, identifying landmarks, segmentation, classification
of voxels for, e.g., cancer cell detection. Why? Because a single image can
provide a lot of data. For per-image deep learning need a lot of data. Finding
patterns in images, diagnosis. Current models not sensitive enough, but if more
sensitive, they might over fit.

Training data is important. Patch-based approaches successfully, because
generally only a few images to work with and every patch counts as training
data. Fully convolutional approaches also very successful, because every pixel
counts as a training sample. However, higher layers have fewer pixels and
therefore fewer training samples. So deep models can overfit and might require
more data. Full image classification challenging, because not much labelled
data. Two approaches: use unsupervised learning because you can also leverage
unlabelled data. Use unsupervised learning because it tends to overfit less.
Large body on unsupervised feature learning for AD classification.

What can be calculated \citep{london2005}. Multiplications. Auditory system has
neurons that calculate that and these are very important. Might be important for
other tasks as well. Gated RBM models can calculate that. Allows three way
connections. Also allows better model of natural images when putting the same
image in twice. Allows multiplicative connection between images. Other
approaches maybe deep polynomial networks. Or look at gated RBMs and unroll them
to a neural network, if possible. Might show the differences between the two
models more and maybe might show, why generative models might still be a good
idea.

Invariance has been an important factor. Introduction of convolutional models
and pooling greatly reduces the number of weights that need to be learned.
Invariance to rotations might proof important as well. First approaches from
Ronneberger. Might be even more important in 3D due to the much larger space of
rotations.

\end{comment}