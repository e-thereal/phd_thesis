\chapter{White Matter Lesion Segmentation}

\section{Introduction}

Multiple sclerosis (MS) is an inflammatory and demyelinating disease of the
central nervous system with pathology that can be observed in vivo by magnetic
resonance imaging (MRI). MS is characterized by the formation of lesions,
primarily visible in the white matter on conventional MRI. Imaging biomarkers
based on the delineation of lesions, such as lesion load and lesion count, have
established their importance for assessing disease progression and treatment
effect. However, lesions vary greatly in size, shape, intensity and location,
which makes their automatic and accurate segmentation challenging.

\section{Related Work}

Many automatic methods have been proposed for the segmentation of MS
\mbox{lesions} over the last two decades \cite{garcia2013}, which can be
classified into unsupervised and supervised methods.

\subsection{Unsupervised Methods}

Unsupervised methods do not require a labeled data set for training. Instead,
lesions are identified as an outlier of, e.g., a subject-specific generative
model of tissue intensities
\cite{vanleemput2001,tomas2015,schmidt2012automated,roura2015}, or a generative
model of image patches representing a healthy population \cite{weiss2013}.
Alternatively, clustering methods have been used to segment healthy and lesion
tissue, where lesions are modelled as a separate tissue class
\cite{shiee2010topology,sudre2015}. In many methods, spatial priors of healthy
tissues are used to reduce false positives. For example, in addition to
modelling MS lesions as a separate intensity cluster, Lesion-TOADS
\cite{shiee2010topology} employs topological and statistical atlases to produce
a topology-preserving segmentation of all brain tissues.

To account for local changes of the tissue intensity distributions,
Tomas-Fernandez et al. \cite{tomas2015} combined the subject-specific model of
the global intensity distributions with a voxel-specific model calculated from a
healthy population, where lesions are detected as outliers of the combined
model. A major challenge of unsupervised methods is that outliers are often not
specific to lesions and can also be caused by intensity inhomogeneities, partial
volume, imaging artifacts, and small anatomical structures such as blood
vessels, which leads to the generation of false positives. To overcome these
limitations, Roura et al. \cite{roura2015} employed an additional set of rules
to remove false positives, while Schmidt et al. \cite{schmidt2012automated} used
a conservative threshold for the initial detection of lesions, which are later
grown in a separate step to yield an accurate delineation.

\subsection{Supervised Methods}

Current supervised approaches typically start with a set of features, which can
range from small and simple to large and highly variable, and are either predefined
by the user \cite{geremia2010,guizard2015,subbanna2015} or gathered in a feature
extraction step such as by deep learning \cite{yoo2014}. Voxel-based
segmentation algorithms \cite{geremia2010,yoo2014} feed the features and labels
of each voxel into a general classification algorithm, such as a random forest
\cite{breiman2001}, to classify each voxel and to determine which set of
features are the most important for segmentation in the particular domain.
Voxel features and the labels of neighboring voxels can be incorporated into
Markov random field-based (MRF-based) approaches
\cite{subbanna2009,subbanna2015} to produce a spatially consistent segmentation.
As a strategy to reduce false positives, Subbanna et al.
\cite{subbanna2015} combined a voxel-level MRF with a regional MRF, which
integrates a large set of intensity and textural features extracted from the
regions produced by the voxel-level MRF with the labels of neighboring nodes of
the regional MRF.
Library-based approaches leverage a library of pre-segmented images to carry out
the segmentation. For example, Guizard et al.
\cite{guizard2015} proposed a segmentation method based on an extension of the
non-local means algorithms \cite{coupe2011}. The centers of patches at every
voxel location are classified based on matched patches from a library containing
pre-segmented images, where multiple matches are weighted using a similarity
measure based on rotation-invariant features.

\subsection{Patch-based Deep Learning Methods}

A recent breakthrough for the automatic segmentation using deep learning comes from
the domain of cell membrane segmentation, in which Cire\c{s}an et al.
\cite{Ciresan2012} proposed classifying the centers of image patches directly
using a convolutional neural network (CNN) \cite{LeCun1998} without a dedicated
feature extraction step. Instead, features are learned indirectly within the
lower layers of the neural network during training, while the higher layers can
be regarded as performing the classification, which allows the learning of
features that are specifically tuned to the segmentation task. However, the time
required to train patch-based methods can make the approach infeasible when the
size and number of patches are large.

\subsection{Fully Convolutional Methods}

Recently, different CNN architectures
\cite{long2015,ronneberger2015,brosch2015,kang2014fully} have been proposed that
are able to feed through entire images, which removes the need to select
representative patches, eliminates redundant calculations where patches overlap,
and therefore these models scale up more efficiently with image resolution. Kang
et al. introduced the fully convolutional neural network (fCNN) for the
segmentation of crowds in surveillance videos \cite{kang2014fully}. However, fCNNs produce
segmentations of lower resolution than the input images due to the successive
use of convolutional and pooling layers, both of which reduce the
dimensionality.
To predict segmentations of the same resolution as the input images, we recently
proposed using a 3-layer convolutional encoder network (CEN) \cite{brosch2015}
for MS lesion segmentation. The combination of convolutional \cite{LeCun1998}
and deconvolutional \cite{zeiler2011} layers allows our network to produce
segmentations that are of the same resolution as the input images.

Another limitation of the traditional CNN is the trade-off between localization
accuracy, represented by lower-level features, and contextual information,
provided by higher-level features. To overcome this limitation, Long et al.
\cite{long2015} proposed fusing the segmentations produced by the lower layers of the network
with the upsampled segmentations produced by higher layers. However, using only
low-level features was not sufficient to produce a good segmentation at the
lowest layers, which is why segmentation fusion was only performed for the three
highest layers.
Instead of combining the segmentations produced at
different layers, Ronneberger et al. \cite{ronneberger2015} proposed combining
the features of different layers to calculate the final segmentation
directly at the lowest layer using an 11-layer u-shaped network architecture
called u-net. Their network is
composed of a traditional contracting path (first half of the u), but augmented
with an expanding path (last half of the u), which replaces the pooling layers
of the contracting path with upsampling operations. To leverage both high- and
low-level features, shortcut connections are added between corresponding layers
of the two paths.
However,
upsampling cannot fully compensate for the loss of resolution, and special
handling of the border regions is still required.

% We have evaluated our method on two widely used publicly available data sets for
% the evaluation of MS lesion segmentation methods and a large in-house data set
% from an MS clinical trial, with a comparison of network architectures of
% different depths and with and without shortcuts\footnote{Where the risk of
% confusion is minimal, we will refer to the shortcut connections between two
% corresponding layers as a single shortcut (see Fig. 1).}.

\section{Methods}
\label{sec:method}

We propose a new convolutional network architecture that combines the advantages
of a CEN \cite{brosch2015} and a u-net \cite{ronneberger2015}. Our
network is divided into two pathways, a traditional convolutional pathway, which
consists of alternating convolutional and pooling layers, and a deconvolutional
pathway, which consists of alternating deconvolutional and unpooling layers and
predicts the final segmentation. Similar to the u-net, we introduce shortcut
connections between layers of the two pathways. In contrast to the u-net, our
network uses deconvolution instead of upsampling in the expanding pathway and
predicts segmentations that have the same resolution as the input images and
therefore does not require special handling of the border regions.

\begin{figure*}[tb]
\centering
\input{tikzfigures/encoder_L2}

\caption{Pre-training and fine-tuning of the 7-layer convolutional encoder
network with shortcut that we used for our experiments. Pre-training is
performed on the input images using a stack of convolutional RBMs. The
pre-trained weights and bias terms are used to initialize a convolutional
encoder network, which is fine-tuned on pairs of input images, $x^{(0)}$, and
segmentations, $y^{(0)}$.}

\label{fig:network}
\end{figure*}

\subsection{Segmentation as an Optimization Problem}

In this paper, the task of segmenting MS lesions is defined as finding a
function $s$ that maps multi-modal images $I$, e.g., $I = (I_\text{FLAIR},
I_\text{T1})$, to corresponding binary lesion masks $S$, where $1$ denotes a
lesion voxel and $0$ denotes a non-lesion voxel. Given a set of training images
$I_n$, $n \in \N$, and corresponding segmentations $S_n$, we model finding an
appropriate function for segmenting MS lesions as an optimization problem of the
following form
\begin{equation}
\hat{s} = \arg \min_{s \in \mathcal{S}} \sum_n E(S_n, s(I_n)),
\label{eq:segprob}
\end{equation}
where $\mathcal{S}$ is the set of possible segmentation functions, and $E$ is an
error measure that calculates the dissimilarity between ground truth
segmentations and predicted segmentations.

\subsection{Model Architecture}

The set of possible segmentation functions, $\mathcal{S}$, is modeled by the
convolutional encoder network with shortcut connections (CEN-s) illustrated in
Fig.~\ref{fig:network}. A CEN-s is a type of convolutional neural network (CNN)
\cite{LeCun1998} that is divided into two interconnected pathways, the
convolutional pathway and the deconvolutional \cite{zeiler2011} pathway. The
convolutional pathway consists of alternating convolutional and pooling layers.
The input layer of the convolutional pathway is composed of the image voxels
$x^{(0)}_i(\vect{p})$, $i \in [1, C]$, where $i$ indexes the modality or input
channel, $C$ is the number of modalities or channels, and $\vect{p} \in \N^3$
are the coordinates of a particular voxel. The convolutional layers
automatically learn a feature hierarchy from the input images. A convolutional
layer is a deterministic function of the following form
\begin{equation}
x^{(l)}_j = \max \Bigg(0, \sum_{i=1}^C\tilde{w}^{(l)}_{\text{c},ij}*x^{(l-1)}_i
+ b^{(l)}_j\Bigg),
\end{equation}
where $l$ is the index of a convolutional layer, $x^{(l)}_j$, $j \in [1,F]$,
denotes the feature map corresponding to the trainable convolution filter
$w^{(l)}_{\text{c},ij}$, $F$ is the number of filters of the current layer,
$b^{(l)}_j$ are trainable bias terms, $*$ denotes valid convolution, and
$\tilde{w}$ denotes a flipped version of $w$, i.e., $\tilde{w}(a) = w(-a)$. To
be consistent with the inference rules of convolutional restricted Boltzmann
machines (convRBMs) \cite{lee2009convolutional}, which are used for
pre-training, convolutional layers convolve the input signal with flipped filter
kernels, while deconvolutional layers calculate convolutions with non-flipped
filter kernels. We use rectified linear units \cite{nair2010} in all layers
except for the output layers, which have shown to improve the classification
performance of CNNs \cite{krizhevsky2012}. A convolutional layer is followed by
an average pooling layer \cite{scherer2010evaluation} that halves the number of
units in each dimension by calculating the average of each block of \num{2x2x2}
units per channel.

The deconvolutional pathway consists of alternating deconvolutional and
unpooling layers with shortcut connections to the corresponding
convolutional layers. The first deconvolutional layer uses the extracted
features of the convolutional pathway to calculate abstract segmentation
features
\begin{equation}
y^{(L-1)}_i = \max\Bigg(0, \sum_{j=1}^Fw^{(L)}_{\text{d},ij}\circledast
y^{(L)}_j + c^{(L-1)}_{i}\Bigg),
\end{equation}
where $y^{(L)} = x^{(L)}$, $L$ denotes the number of layers of the convolutional
pathway, $w^{(L)}_{\text{d},ij}$ and $c^{(L-1)}_i$ are trainable parameters of
the deconvolutional layer, and $\circledast$ denotes full convolution. To be
consistent with the general notation of deconvolutions \cite{zeiler2011}, the
non-flipped version of $w$ is convolved with the input signal.

Subsequent
deconvolutional layers use the activations of the previous layer
and corresponding convolutional layer to calculate more localized segmentation
features
\begin{equation}
y^{(l)}_i = \max\Bigg(0, 
\sum_{j=1}^Fw^{(l+1)}_{\text{d},ij}\circledast y^{(l+1)}_j
+ \sum_{j=1}^F w^{(l+1)}_{\text{s},ij}\circledast x^{(l+1)}_j +
c^{(l)}_i\Bigg),
\end{equation}
where $l$ is the index of a deconvolutional layer with shortcut, and
$w^{(l+1)}_{\text{s},ij}$ are the shortcut filter kernels connecting the
activations of the convolutional pathway with the activations of the
deconvolutional pathway. The last deconvolutional layer integrates the low-level
features extracted by the first convolutional layer with the high-level features
from the previous layer to calculate a probabilistic lesion mask
\begin{equation}
y^{(0)}_1 = \sigm\Bigg(\sum_{j=1}^F\Big(w^{(1)}_{\text{d},1j}\circledast
y^{(1)}_j +
w^{(1)}_{\text{s},1j}\circledast x^{(1)}_j\Big) + c^{(0)}_1\Bigg),
\end{equation}
where we use the sigmoid function defined as $\sigm(z) = (1 + \exp(-z))^{-1}, z
\in \R$ instead of the rectified linear function in order to obtain a
probabilistic segmentation with values in the range between 0 and 1.
To produce a binary lesion mask from the probabilistic output of our model, we
chose a fixed threshold such that the mean Dice similarity coefficient
\cite{dice1945measures} is maximized on the training set and used the same
threshold for the evaluation on the test set.

\subsection{Gradient Calculation}

The parameters of the model can be efficiently learned by minimizing the error
$E$ for each sample of the training set, which requires the calculation of the
gradient of $E$ with respect to the model parameters \cite{LeCun1998}.
Typically, neural networks are trained by minimizing the sum of squared
differences (SSD), which can be calculated for a single image as follows
\begin{equation}
% Error function
E = \frac{1}{2}\sum_{\vect{p}}\left(S(\vect{p}) -
y^{(0)}(\vect{p})\right)^2,
\end{equation}
where $\vect{p} \in \N^3$ are the coordinates of a particular voxel.
The partial derivatives of the error with respect to the model parameters can be
calculated using the delta rule and are given by 
\begin{align}
\label{eq:dEd}
% Deconvolutional filters
\frac{\partial E}{\partial w^{(l)}_{\text{d},ij}} &=
\delta^{(l-1)}_{\text{d},i} * \tilde{y}^{(l)}_j, &
% Deconvolutional bias
\frac{\partial E}{\partial c^{(l)}_i} &= \sum_{\vect{p}}
\delta^{(l)}_{\text{d},i}(\vect{p}), \\
\label{eq:dEs}
% Shortcut filters
\frac{\partial E}{\partial w^{(l)}_{\text{s},ij}}
 &= \delta^{(l-1)}_{\text{d},i} * \tilde{x}^{(l)}_j, \\
 \label{eq:dEc}
% Convolutional filters
\frac{\partial E}{\partial w^{(l)}_{\text{c},ij}} 
&= x^{(l-1)}_i * \tilde{\delta}^{(l)}_{\text{c},j},\text{ and}&
% Convolutional bias
\frac{\partial E}{\partial b^{(l)}_i} &= \sum_{\vect{p}}
\delta^{(l)}_{\text{c},i}(\vect{p}).
\end{align}
For the first layer, $\delta^{(0)}_{\text{d},1}$ can be calculated by
\begin{equation}
% Delta update
\delta^{(0)}_{\text{d},1} = \big(y^{(0)}_1
-S\big)y^{(0)}_1\big(1-y^{(0)}_1\big).
\label{eq:delta0}
\end{equation}
The derivatives of the error with respect to the parameters of the other layers
can be calculated by applying the chain rule of partial derivatives, which
yields to
\begin{align}
\label{eq:deltad}
% Delta update of deconvolutional layer
\delta^{(l)}_{\text{d},j} &=
\big(\tilde{w}^{(l)}_{\text{d},ij}*\delta^{(l-1)}_{\text{d},i}\big)\I\big(y^{(l)}_j
> 0\big),\\
\label{eq:deltac}
% Delta update of convolutional layer
\delta^{(l)}_{\text{c},i} &=
\big(w^{(l+1)}_{\text{c},ij}\circledast\delta^{(l+1)}_{\text{c},j}\big)\I\big(x^{(l)}_i
> 0\big),
\end{align}
where $l$ is the index of a deconvolutional or convolutional layer,
$\delta^{(L)}_{\text{c},i} = \delta^{(L)}_{\text{d},j}$, and $\I(z)$ denotes the
indicator function defined as $1$ if the predicate $z$ is true and $0$
otherwise. If a layer is connected through a shortcut,
$\delta^{(l)}_{\text{c},j}$ needs to be adjusted by propagating the error back
through the shortcut connection. In this case, $\delta^{(l)}_{\text{c},j}$ is
calculated by
\begin{equation}
% Delta update convolutional with shortcut
\delta^{(l)}_{\text{c},j} =
\big(\delta^{(l)}_{\text{c},j}{}'+
\tilde{w}^{(l)}_{\text{s},ij}*\delta^{(l-1)}_{\text{d},i}\big)\I\big(x^{(l)}_j
> 0\big),
\label{eq:deltas}
\end{equation}
where $\delta^{(l)}_{\text{c},j}{}'$ denotes the activation of unit
$\delta^{(l)}_{\text{c},j}$ before taking the shortcut connection into account.

The sum of squared differences is a good measure of classification accuracy, if
the two classes are fairly balanced. However, if one class contains vastly more
samples, as is the case for lesion segmentation, the error measure is dominated
by the majority class and consequently, the neural network would learn to ignore
the minority class. To overcome this problem, we use a combination of
sensitivity and specificity, which can be used together to measure
classification performance even for vastly unbalanced problems. More precisely,
the final error measure is a weighted sum of the mean squared difference of the
lesion voxels (sensitivity) and non-lesion voxels (specificity), reformulated to
be error terms:
\begin{equation} 
E = r\frac{\textstyle\sum_{\vect{p}} \left(S(\vect{p}) -
y^{(0)}(\vect{p})\right)^2 S(\vect{p})}{\textstyle\sum_{\vect{p}} S(\vect{p})}
 + (1-r)\frac{\textstyle\sum_{\vect{p}} \left(S(\vect{p}) -
y^{(0)}(\vect{p})\right)^2 \big(1 - S(\vect{p})\big)}{%
\textstyle\sum_{\vect{p}}\big(1 - S(\vect{p})\big)}.
\end{equation}
We formulate the sensitivity and specificity errors as squared errors in order
to yield smooth gradients, which makes the optimization more robust. The
sensitivity ratio $r$ can be used to assign different weights to the two terms.
Due to the large number of non-lesion voxels, weighting the specificity error
higher is important, but based on preliminary experimental results \cite{brosch2015},
the algorithm is stable with respect to changes in $r$, which largely affects the
threshold used to binarize the probabilistic output. A detailed evaluation of
the impact of the sensitivity ratio on the learned model is presented in
Section~III-D.

To train our model, we must compute the derivatives of the modified objective
function with respect to the model parameters. Equations
\ref{eq:dEd}--\ref{eq:dEc} and \ref{eq:deltad}--\ref{eq:deltas} are a
consequence of the chain rule and independent of the chosen similarity measure.
Hence, we only need to derive the update rule for $\delta^{(0)}_{\text{d},1}$.
With $\alpha = 2r (\sum_{\vect{p}}S(\vect{p}))^{-1}$ and $\beta = 2(1 -
r)(\sum_{\vect{p}}(1 - S(\vect{p})))^{-1}$, we can rewrite $E$ as
\begin{align}
E=& \frac{1}{2}\sum_{\vect{p}}\big(\alpha S(\vect{p}) +
\beta(1-S(\vect{p}))\big)\Big(S(\vect{p})-y^{(0)}_1(\vect{p})\Big)^2.
\end{align}
%
Our objective function is similar to the SSD, with an additional multiplicative
term applied to the squared differences. The additional factor is constant with
respect to the model parameters. Consequently, $\delta^{(0)}_{\text{d},1}$ can
be derived analogously to the SSD case, and the new factor is simply carried
over:
\begin{equation} 
\delta^{(0)}_{\text{d},1} = \big(\alpha S + \beta (1 - S)\big)\big(y^{(0)}_1 -
S\big) y^{(0)}_1 \big(1 - y^{(0)}_1\big).
\end{equation}

\subsection{Training}

At the beginning of the training procedure, the model parameters need to be
initialized and the choice of the initial parameters can have a big impact on
the learned model \cite{sutskever2013importance}. In our experiments, we found
that initializing the model using pre-training \cite{Hinton2006} on the input images was required in order
to be able to fine-tune the model using the ground truth segmentations without
getting stuck early in a local minimum. Pre-training can be performed layer by
layer \cite{Hinton2006b} using a stack of convRBMs (see Fig.~\ref{fig:network}),
thereby avoiding the potential problem of vanishing or exploding gradients
\cite{hochreiter1991untersuchungen}. The first convRBM is trained on the input
images, while subsequent convRBMs are trained on the hidden activations of the
previous convRBM. After all convRBMs have been trained, the model parameters of
the CEN-s can be initialized as follows (showing the first convolutional and
the last deconvolutional layers only, see Fig.~\ref{fig:network})
\begin{align}
w_{\text{c}}^{(1)} &= \hat{w}^{(1)}, &
w_{\text{d}}^{(1)} &= 0.5\hat{w}^{(1)}, &
w_{\text{s}}^{(1)} &= 0.5\hat{w}^{(1)} \\
b^{(1)} &= \hat{b}^{(1)}, &
c^{(0)} &= \hat{c}^{(1)},
\end{align}
where $\hat{w}^{(1)}$ are the filter weights, $\hat{b}^{(1)}$ are the hidden
bias terms, and $\hat{c}^{(1)}$ are the visible bias terms of the first convRBM.

A major challenge for gradient-based optimization methods is the choice of an
appropriate learning rate. Classic stochastic gradient descent \cite{LeCun1998}
uses a fixed or decaying learning rate, which is the same for all parameters of
the model. However, the partial derivatives of parameters of different layers
can vary substantially in magnitude, which can require different learning rates.
In recent years, there has been an increasing interest in developing methods for
automatically choosing independent learning rates. Most methods (e.g., AdaGrad
\cite{duchi2011adaptive}, AdaDelta \cite{zeiler2012adadelta}, RMSprop
\cite{dauphin2015rmsprop}, and Adam \cite{kingma2014adam}) collect different
statistics of the partial derivatives over multiple iterations and use this
information to set an adaptive learning rate for each parameter. This is
especially important for the training of deep networks, where the optimal
learning rates often differ greatly for each layer. In our initial experiments,
networks obtained by training with AdaDelta, RMSprop, and Adam performed
comparably well, but AdaDelta was the most robust to the choice of
hyperparameters, so we used AdaDelta for all results reported.

\subsection{Implementation}

% TODO: Implementation outlined in a previous section

Pre-training and fine-tuning were performed using a highly optimized
GPU-accelerated implementation of 3D convRBMs and CENs that performs training
in the frequency domain \cite{brosch2014efficient}. Our frequency domain
implementation significantly speeds up the training by mapping the calculation
of convolutions to simple element-wise multiplications, while adding only a
small number of Fourier transforms. This is especially beneficial for the
training on 3D volumes, due to the increased number of weights of 3D kernels
compared to 2D. Although GPU-accelerated deep learning libraries based on cuDNN
\cite{chetlur2014} are publicly available (e.g.,
\cite{jia2014,Bastien2012,collobert2011torch7}), we trained our models using our
own implementation because we have found that it performs the most
computationally intensive training operations 6 times faster than cuDNN in a
direct comparison.
 
\sisetup{separate-uncertainty=true,detect-weight=true,detect-inline-weight=math}

\section{Experiments and Results}

% TODO: Add MICCAI results

We evaluated our method on two publicly available data sets (from the MICCAI
2008 and ISBI 2015 lesion segmentation challenges), which allows for a direct
comparison with many state-of-the-art methods. In addition, we have used a much
larger data set containing four different MRI sequences from a multi-center
clinical trial in relapsing remitting MS, which tends to have the most
heterogeneity among the MS subtypes. This data test is challenging due to the
large variability in lesion size, shape, location, and intensity as well as
varying contrasts produced by different scanners. The clinical trial data set
was used to carry out a detailed analysis of different CEN architectures using
different combinations of modalities, with a comparison to five publicly
available state-of-the-art methods.

\subsection{Data Sets and Pre-processing}

\subsubsection{Public data sets}
The data set of the MICCAI 2008 MS lesion segmentation challenge
\cite{styner20083d} consists of 43 T1-weighted (T1w), T2-weighted (T2w), and
FLAIR MRIs, divided into 20 training cases for which ground truth segmentations
are made publicly available, and 23 test cases. After training the model on the
20 training cases, we used the trained model to segment the 23 test cases, which
were sent to the challenge organizers for independent evaluation.

The data set of the ISBI 2015 longitudinal MS lesion segmentation challenge
consists of 21 visit sets, each with T1w, T2w, proton density-weighted (PDw),
and FLAIR MRIs. The challenge was not open for new submissions at the time of
writing this article. Therefore, we evaluated our method on the training set
using leave-one-subject-out cross-validation, following the evaluation protocol
of the second \cite{jesson2015} and third place \cite{maier2015} methods from
the challenge proceedings. The paper of the first place method does not have
sufficient details to replicate their evaluation.


\subsubsection{Clinical trial data set}

The data set was collected from 67 different scanning sites using different
1.5\,T and 3\,T scanners, and consists of T1w, T2w, PDw, and FLAIR MRIs from 195
subjects, most with two time points (377 visit sets in total). The image
dimensions and voxel sizes vary by site, but most of the T1w images have close
to \SI{1}{\milli\meter} isotropic voxels, while the other images have voxel
sizes close \SI{1x1x3}{\milli\meter}.
All images were skull-stripped using the brain extraction tool (BET)
\cite{jenkinson2005bet2}, followed by an intensity normalization to the interval
$[0,1]$, and a 6 degree-of-freedom intra-subject registration using one of the
\SI{3}{\milli\meter} scans as the target image to align the different
modalities. To speed-up the training, all images were cropped to a
\num{164x206x52} voxel subvolume with the brain roughly centered. The ground
truth segmentations were produced using an existing semiautomatic 2D
region-growing technique, which has been used successfully in a number of large
MS clinical trials (e.g., \cite{kappos2006long},
\cite{traboulsee2008reduction}). Each lesion was manually identified by an
experienced radiologist and then interactively grown from the seed point by a
trained technician.

We divided the data set into a training ($n=250$), validation ($n=50$), and test
set ($n=77$) such that images of each set were acquired from different scanning
sites. The training, validation, and test sets were used for training our
models, for monitoring the training progress, and to evaluate performance,
respectively. The training set was also used to perform parameter tuning of the
other methods used for comparison. Pre-training and fine-tuning of our 7-layer
CEN-s took approximately 27 hours and 37 hours, respectively, on a single
GeForce GTX 780 graphics card. Once the network is trained, new multi-contrast
images can be segmented in less than one second.


\subsection{Comparison to Other Methods}

We compared our method with the five publicly available methods, some of which are widely used
for clinical research and are established in the literature (e.g.,
\cite{sudre2015,subbanna2015,guizard2015}) as reference points for comparison. These five methods include:
\begin{enumerate}
\item Expectation maximization segmentation (EMS) method \cite{vanleemput2001};
\item Lesion growth algorithm (LST-LGA) \cite{schmidt2012automated}, as
implemented in the Lesion Segmentation Toolbox (LST) version 2.0.11;
\item Lesion prediction algorithm (LST-LPA) also implemented in the same LST toolbox;
\item Lesion-TOADS version 1.9 R \cite{shiee2010topology}; and 
\item Salem Lesion Segmentation (SLS) toolbox \cite{roura2015}.
\end{enumerate}
 
The Lesion-TOADS software only takes T1w and FLAIR MRIs and has no tunable
parameters, so we used the default parameters to carry out the segmentations.
The performance of EMS depends on the choice of the Mahalanobis distance
$\kappa$, the threshold $t$ used to binarize the probabilistic segmentation, and
the modalities used. We applied EMS to segment lesions using two combinations of
modalities: a) T1w, T2w, and PDw, as used in the original paper
\cite{vanleemput2001}, and b) all four available modalities (T1w, T2w, PD2,
FLAIR). We compared the segmentations produced for all combinations of $\kappa =
2.0, 2.2, \dotsc, 4.6$ and $t = 0.05, 0.10, \dotsc, 1.00$ with the ground truth
segmentations on the training set and chose the values that maximized the
average DSC ($\kappa = 2.6, t = 0.75$ for three modalities; $\kappa = 2.8, t =
0.9$ for four modalities).

The LST-LGA and LST-LPA of the LST toolbox only take T1w and FLAIR MRIs as
input, and we used those modalities to tune the initial threshold $\kappa$ of
LST-LGA for $\kappa = 0.05, 0.10, \dotsc, 1.00$ and the threshold $t$ used by
LST-LPA to binarize the probabilistic segmentations for $t = 0.05, 0.10, \dotsc,
1.00$. The optimal parameters were $\kappa = 0.10$ and $t = 0.45$, respectively.

Similarly, the SLS toolbox, which is the most recently published work
\cite{roura2015} also only takes T1w and FLAIR MRIs as input. This method uses
an initial brain tissue segmentation obtained from the T1w images and segments
lesions by treating lesional pixels as outliers to the normal appearing GM brain
tissue on the FLAIR images. This method has three key parameters:
$\omega_\text{nb}$, $\omega_\text{ts}$, and $\alpha_\text{sls}$. We tuned these
parameters on the training set via a grid-search over a range of values as
suggested in \cite{roura2015}.

\subsection{Measures of Segmentation Accuracy}
 
We used the following four measures to produce a comprehensive evaluation
of segmentation accuracy as there is generally no single measure that is
sufficient to capture all information relevant to the quality of a produced
segmentation \cite{garcia2013review}.

The first measure is the Dice similarity coefficient (DSC)
\cite{dice1945measures} that computes a normalized overlap value between the
produced and ground truth segmentations, and is defined as
\begin{equation}
\text{DSC} = \frac{2 \times \text{TP}}{2 \times \text{TP} + \text{FP} +
\text{FN}},
\end{equation}
where TP, FP, and FN denote the number of true positive, false positive, and
false negative voxels, respectively. A value of \SI{100}{\percent} indicates a perfect
overlap of the produced segmentation and the ground truth.
The DSC incorporates measures of over- and underestimation into a single
metric, which makes it a suitable measure to compare overall segmentation
accuracy. 

We also measured the relative absolute volume difference between the
ground truth and the produced segmentation by computing their volumes
($\text{\textit{Vol}}$), i.e.,
\begin{equation}
\text{VD} = \frac{\text{\textit{Vol}}(\text{\textit{Seg}}) -
                    \text{\textit{ Vol}}(\text{\textit{GT}})}
                {\text{\textit{ Vol}}(\text{\textit{GT}})},
\end{equation}
where $Seg$ and $GT$ denote the obtained segmentation and ground truth,
respectively. However, it has been noted \cite{garcia2013review} that wide
variability exists even between the lesion segmentations of trained experts, and
thus, the achieved volume differences reported in the literature have ranged
from \SI{10}{\percent} to \SI{68}{\percent}.

For more precise evaluation, we have also included the lesion-wise true positive
rate (LTPR) and the lesion-wise false positive rate (LFPR) that are much more
sensitive in measuring the segmentation accuracy of smaller lesions, which are
important to detect when performing early disease diagnosis
\cite{garcia2013review}.
More specifically, the LTPR measures the true positive rate (TPR) on a per
lesion-basis and is defined as
\begin{equation}
\text{LTPR} = \frac{\text{LTP}}{\text{\#RL}},
\end{equation}
where LTP denotes the number of lesion true positive, i.e., the number
lesions in the reference segmentation that overlap with a lesion in the produced
segmentation, and \#RL denotes the total number of lesions in the reference
segmentation. An LTPR with a value of \SI{100}{\percent} indicates that all
lesions are correctly identified. Similarly, the lesion-wise false positive rate
(FPR) measures the fraction of the segmented lesions that are not in the ground
truth and is defined as
\begin{equation}
\text{LFPR} = \frac{\text{LFP}}{\text{\#PL}},
\end{equation}
where LFP denotes the number of lesion false positives, i.e., the number of
lesions in the produced segmentation that do not overlap with a lesion in the
reference segmentation, and \#PL denotes the total number of lesions in the
produced segmentation. An LFPR with a value of \SI{0}{\percent} indicates that
no lesions were incorrectly identified.

\subsection{Training Parameters}
The most influential parameters of the training method are the number of epochs
and the sensitivity ratio. Fig.~\ref{fig:epochs} shows the mean DSC evaluated on
the training and validation sets of the clinical data set as computed during
training of a \mbox{7-layer} CEN-s up to 500 epochs.
The mean DSC scores increase monotonically, but the improvements are minor after
400 epochs. The optimal number of epochs is a trade-off between accuracy and
time required for training. Due to the relatively small improvements after 400
epochs, we decided to stop the training procedure at 500 epochs. For the
challenge data sets, due to their small sizes, we did not employ a subset of the
data for a dedicated validation set to choose the number of epochs. Instead, we
set the number of epochs to 2500, which corresponds to roughly the same number
of gradient updates compared to the clinical trial data set.

\begin{figure}
\centering
\includegraphics[width=\textwidth]{figures/tmi/ems_progress}
\caption{Improvement in the mean DSC computed on the training and test sets
during training. Only small improvements can be observed after 400 epochs.}
\label{fig:epochs}
\end{figure}

To determine an effective sensitivity ratio, we measured the performance on the
validation set over a range of values. For each choice of ratio, we binarized
the segmentations using a threshold that maximized the DSC on the training set.
Fig.~\ref{fig:ratio} shows a set of ROC curves for different choices of the
sensitivity ratio ranging from 0.01 to 0.10 and the corresponding optimal
thresholds. The plots illustrate our findings that our method is not sensitive
to the choice of the sensitivity ratio, which mostly affects the optimal
threshold. We chose a fixed sensitivity ratio of 0.02 for all our experiments.

\begin{figure}
\centering
\includegraphics[width=\textwidth]{figures/tmi/roc2}
\caption{ROC curves for different sensitivity ratios $r$. A `$+$' marks the TPR
and FPR of the optimal threshold. Varying the value of $r$ results in almost
identical ROC curves and only causes a change of the optimal threshold $t$,
which shows the robustness of our method with respect to the sensitivity ratio.}
\label{fig:ratio}
\end{figure}

\subsection{Comparison on Public Data Sets}

To allow for a direct comparison with a large number of state-of-the-art
methods, we evaluated our method on the MICCAI 2008 MS lesion segmentation
challenge \cite{styner20083d} and the ISBI 2015 longitudinal MS lesion
segmentation challenge. We have previously shown that approximately 100 images
are required to train the 3-layer CEN without overfitting \cite{brosch2015} and
we expect the required number of images to be even higher when adding more layers.
Due to the relatively small size of the training data sets provided by the two
challenges, we used a CEN with only three layers on these data sets to reduce
the risk of overfitting. The parameters of the models are summarized in
Table~\ref{tab:archchallenge}.

\begin{table}[tb]
\caption{Parameters of the 3-layer CEN for the evaluation on the challenge data
sets.}
\label{tab:archchallenge}
\begin{center}
\begin{tabular}{@{}lccr@{}}
\toprule
Layer type & Kernel Size & \#Filters & \multicolumn{1}{c}{Image Size} \\
\midrule
Input & --- & --- & $164\times 206\times 156\times c$\phantom{0} \\
Convolutional & $9\times 9\times 9\times c$\phantom{0} & 32 &
\num{156x198x148x32} \\
Deconvolutional & \num{9x9x9x32} & 1 & \num{164x206x156x1}\phantom{0} \\
\bottomrule
\end{tabular}
\end{center}
Note: The number of input channels $c$ is 3 for the MICCAI challenge and 4 for
the ISBI challenge.
\end{table}

A comparison of our method with other state-of-the-art methods evaluated on the
MICCAI challenge test data set is summarized in Table~\ref{tab:miccai}. Our
method ranked 6th (2nd if only considering methods with only one submission,
i.e., without subsequent parameter-tuning and adjustments) out of 52 entries
submitted to the challenge, outperforming the recent SLS by Roura
et al. \cite{roura2015}, and popular methods such as the random forest approach
by Geremia et al. \cite{geremia2010}, and Lesion-TOADS by Shiee et al.
\cite{shiee2010topology}, but not as well as the patch-based segmentation
approach by Guizard et al. \cite{guizard2015}, or the MOPS approach by
Tomas-Fernandez et al. \cite{tomas2015}, which used additional images to build
the intensity model of a healthy population. This is a very promising result for
the first submission of our method given the simplicity of the model and the
small training set size.

\begin{table}
\sisetup{
  round-mode = places,
  round-precision = 2}%
\caption{Selected methods out of the 52 entries submitted for evaluation to the
MICCAI 2008 MS lesion segmentation challenge.}
\label{tab:miccai}
\begin{center}
\begin{tabular}{@{}clS[table-format=2.2]
S[table-format=2.1,round-precision=1]
S[table-format=2.1,round-precision=1]
S[table-format=2.1,round-precision=1]@{}}
\toprule
Rank & Method & {Score} & {LTPR} & {LFPR} & {VD} \\
\midrule
$1,3,9$  & Jesson et al. \cite{jesson2015} & 86.9386 & 48.70 & 28.25 & 80.15 \\
2  & Guizard et al. \cite{guizard2015}   & 86.1071 & 49.85 & 42.75 & 48.80 \\
$4,20,26$  & Tomas-Fernandez et. al \cite{tomas2015} & 84.464 & 46.9 & 44.6 &
45.60 \\
$5,7$ & Jerman et al. \cite{jerman2015}        & 84.1555 & 65.15 & 63.75 & 77.45 \\
6  & Our method    & 84.0743 & 51.55 & 51.25 & 57.75 \\
11 & Roura et al.   \cite{roura2015} & 82.3442 & 50.15 & 41.85 & 111.60 \\
13 & Geremia et al. \cite{geremia2010}     & 82.0691 & 55.1 & 74.1 & 48.90 \\
24 & Shiee et al. \cite{shiee2010topology} & 79.8975 & 52.4 & 72.7 & 74.45 \\
\bottomrule
\end{tabular}
\end{center}
Note: Columns LTPR, LFPR, and VD show the average computed from the two raters in
percent. Challenge results last updated: Dec 15, 2015.
\end{table}

In addition, we evaluated our method on the 21 publicly available labeled cases
from the ISBI 2015 longitudinal MS lesion segmentation challenge. The challenge
organizers have only released the names of the top three teams, only two of
which have published a summary of their mean DSC, LTPR, and LFPR scores for both
raters to allow for a direct comparison. Following the evaluation protocol of
the second \cite{jesson2015} and third \cite{maier2015} place methods, we
trained our model using leave-one-subject-out cross-validation on the training images and compared our
results to the segmentations provided by both raters. Table~\ref{tab:isbi}
summarizes the performance of our method, the two other methods for comparison,
and the performance of the two raters when compared against each other. Compared
to the second and third place methods, our method was more sensitive and
produced significantly higher LTPR scores, but also had more false positives,
which resulted in slightly lower but still comparable DSC scores. This is again
a promising result on a public data set.

\begin{table}
\caption{Comparison of our method with the second and third ranked methods from
the ISBI MS lesion segmentation challenge.}
\label{tab:isbi}
\begin{center}
\begin{tabular}{@{}lcccccc@{}}
\toprule
Method &
\multicolumn{3}{c}{Rater 1} &
\multicolumn{3}{c}{Rater 2} \\
& DSC & LTPR & LFPR & DSC & LTPR & LFPR \\
\midrule
Rater 1 & --- & --- & --- & 73.2 & 64.5 & 17.4 \\
Rater 2 & 73.2 & 82.6 & 35.5 & --- & --- & --- \\
Jesson et al. \cite{jesson2015} &  70.4 & 61.1 & 13.5 & 68.1 & 50.1 & 12.7 \\
Maier et al. (GT1) & 70 & 53 & 48 & 65 & 37 & 44 \\
Maier et al. (GT2) & 70 & 55 & 48 & 65 & 38 & 43 \\
Our method (GT1) & 68.4 & 74.5 & 54.5 & 64.4 & 63.0 & 52.8 \\
Our method (GT2) & 68.3 & 78.3 & 64.5 & 65.8 & 69.3 & 61.9 \\
\bottomrule
\end{tabular}
\end{center}
Note: The evaluation was performed on the training set using
leave-one-subject-out cross-validation. GT1 and GT2 denote that the model was
trained with the segmentations provided by the first and second rater as the
ground truth, respectively.
\end{table}

\subsection{Comparison of Network Architectures, Input Modalities, and
Publicly Available Methods on Clinical Trial Data}

To determine the effect of network architecture, we compared the segmentation
performance of three different networks using T1w and FLAIR MRIs.
Specifically, we trained a 3-layer CEN and two 7-layer CENs, one with shortcut
connections and one without. To investigate the effect of different input image
types, we additionally trained two 7-layer CEN-s on the modalities used by EMS
(T1w, T2w, PDw) and all four modalities (T1w, T2w, PDw, FLAIR). The parameters
of the networks are given in Table~\ref{tab:arch3} and Table~\ref{tab:arch7}. To
roughly compensate for the anisotropic voxel size of the input images, we chose
an anisotropic filter size of \num{9x9x5}.

In addition, we ran the five competing methods discussed in Section~III-B with
Lesion-TOADS, SLS, and the two LST methods using the T1w and FLAIR images, and
EMS using three (T1w, T2w, PDw) and all four modalities in separate tests. A
comparison of the segmentation accuracy of the trained networks and competing
methods is summarized in Table~\ref{tab:results1}.

All CEN architectures performed significantly better than all other methods
regardless of the input modalities, with LST-LGA being the closest in overall
segmentation accuracy. Comparing CEN to LST-LGA, the improvements in the mean
DSC scores ranged from 3 percentage points (pp) for the 3-layer CEN to 17\,pp
for the 7-layer CEN with shortcut trained on all four modalities. The improved
segmentation performance was mostly due to an increase in lesion sensitivity.
LST-LGA achieved a mean lesion TPR of \SI{37.50}{\percent}, compared to
\SI{54.55}{\percent} produced by the CEN with shortcut when trained on the same
modalities, and \SI{62.49}{\percent} when trained on all four modalities, while
achieving a comparable number of lesion false positives. The mean lesion FPRs
and mean volume differences of LST-LGA and the 7-layer CEN-s were very close,
when trained on the same modalities, and the CEN-s further reduced its FPR when
trained on more modalities.

\begin{table}[tb]
\caption{Parameters of the 3-layer CEN used on the clinical trial data set.}
\label{tab:arch3}
\centering
\begin{tabular}{@{}lccr@{}}
\toprule
Layer type & Kernel Size & \#Filters & \multicolumn{1}{c}{Image Size} \\
\midrule
Input & --- & --- & \num{164x206x52x2}\phantom{0} \\
Convolutional & \num{9x9x5x2} & 32 & \num{156x198x48x32} \\
Deconvolutional & \num{9x9x5x32} & 1 & \num{164x206x52x1}\phantom{0} \\
\bottomrule
\end{tabular}
\end{table}

\begin{table}[tb]
\caption{Parameters of the 7-layer CEN-s used on the clinical trial data set.}
\label{tab:arch7}
\centering
\begin{tabular}{@{}lccr@{}}
\toprule
Layer type & Kernel Size & \#Filters & \multicolumn{1}{c}{Image Size} \\
\midrule
Input & --- & --- & \num{164x206x52x2}\phantom{0} \\
Convolutional & \num{9x9x5x2} & 32 & \num{156x198x48x32} \\
{Average Pooling} & \num{2x2x2} & --- & \num{78x99x24x32} \\
{Convolutional} & \num{9x10x5x32} & 32 & \num{70x90x20x32} \\
{Deconvolutional} & \num{9x10x5x32} & 32 & \num{78x99x24x32} \\
{Unpooling }& \num{2x2x2} & --- & \num{156x198x48x32} \\
{Deconvolutional }& \num{9x9x5x32} & 1 & \num{164x206x52x1}\phantom{0} \\
\bottomrule
\end{tabular}
\end{table}


\begin{table}
\begin{center}
\caption{Comparison of the segmentation accuracy of different CEN models, other
methods, and input modalities.}
\label{tab:results1}
\begin{tabular}{@{}lcccc@{}}
\toprule
Method & DSC [\%] & LTPR [\%] & LFPR [\%] & VD [\%] \\
\midrule
\multicolumn{5}{c}{\textit{Input modalities: T1w and FLAIR}} \\
\midrule
3-layer CEN \cite{brosch2015} & 49.24 & 57.33 & 61.39 & 43.45 \\
7-layer CEN & 52.07 & 43.88 & 29.06 & 37.01 \\ 
7-layer CEN-s & 55.76 & 54.55 & 38.64 & 36.30 \\[0.2em]
Lesion-TOADS \cite{shiee2010topology} & 40.04 & 56.56 & 82.90 & 49.36 \\ 
SLS \cite{roura2015}  & 43.20 &  56.80 & 50.80 & 12.30 \\
LST-LGA \cite{schmidt2012automated} & 46.64 & 37.50 & 38.06 & 36.77 \\
LST-LPA \cite{schmidt2012automated} & 46.07 & 48.02 & 52.94 & 41.62 \\
\midrule
\multicolumn{5}{c}{\textit{Input modalities: T1w, T2w, and PDw}} \\
\midrule
7-layer CEN-s & 61.18 & 52.00 & 36.68 & 29.38 \\
EMS \cite{vanleemput2001} & 42.94 & 44.80 & 76.58 & 49.29 \\
\midrule
\multicolumn{5}{c}{\textit{Input modalities: T1w, T2w, FLAIR, and PDw}} \\
\midrule
7-layer CEN-s & 63.83 & 62.49 & 36.10 & 32.89 \\
EMS \cite{vanleemput2001} & 39.70 & 49.08 & 85.01 & 34.51 \\
\bottomrule
\end{tabular}
\end{center}
Note: The table shows the mean of the Dice similarity coefficient (DSC), lesion
true positive rate (LTPR), and lesion false positive rate (LFPR). Because
the volume difference (VD) is not limited to the interval $[0, 100]$, a
single outlier can heavily affect the calculation of the mean. We therefore
excluded outliers before calculating the mean of the VD for all methods using
the box plot criterion.
\end{table}

This experiment also showed that increasing the depth of the CEN
and adding the shortcut connections both improve the segmentation accuracy.
Increasing the depth of the CEN from three layers to seven layers improved the
mean DSC by 3\,pp. The improvement was confirmed to be statistically
significant using a one-sided paired $t$-test ($p$-value of \num{1.3e-5}).
Adding a shortcut to the network further improved the segmentation
accuracy as measured by the DSC by 3\,pp. A second one-sided paired $t$-test
was performed to confirm the statistical significance of the improvement with a
$p$-value of less than \num{1e-10}.

The impact of increasing the depth of the network on the segmentation
performance of very large lesions is illustrated in Fig.~\ref{fig:large}, where
the true positive, false negative, and false positive voxels are highlighted in
green, yellow, and red, respectively. The receptive field of the 3-layer CEN has
a size of only \num{17x17x9} voxels, which reduces its ability to identify very
large lesions marked by two white circles. In contrast, the 7-layer CEN has a
receptive field size of \num{49x53x26} voxels, which allows it to learn features
that can capture much larger lesions. Consequently, the 7-layer CEN, with and
without shortcut, is able to learn a feature set that captures large lesions
much better than the 3-layer CEN, which results in an improved segmentation.
However, increasing the depth of the network without adding shortcut connections
reduces the network's sensitivity to very small lesions as illustrated in
Fig.~\ref{fig:small}. In this example, the 3-layer CEN was able to detect three
small lesions, indicated by the white circles, which were missed by the 7-layer
CEN. Adding shortcut connections enables our model to learn a feature set that
spans a wider range of lesion sizes, which increases the sensitivity to small
lesions and, hence, allows the 7-layer \mbox{CEN-s} to detect all three small
lesions (highlighted by the white circles), while still being able to segment
large lesions.

\begin{figure}
\begin{tikzpicture}[node distance=2.1cm and 0.334\textwidth,
  font=\footnotesize, on grid]
\node[inner sep=0] (image1) {
  \includegraphics[width=\columnwidth]{figures/tmi/p50s35_large_lesions}
};
\node[above=of image1] (l7) {7-layer CEN};
\node[left=of l7] {3-layer CEN};
\node[right=of l7] {7-layer CEN-s};

\begin{scope}[xshift=0.333\textwidth]
\draw[white,thick] (-12pt,-15pt) circle (9pt);
\draw[white,thick] (15pt,-15pt) circle (10pt);
\end{scope}
\begin{scope}[xshift=-0.333\textwidth]
\draw[white,thick] (-12pt,-15pt) circle (9pt);
\draw[white,thick] (15pt,-15pt) circle (10pt);
\end{scope}
\draw[white,thick] (-12pt,-15pt) circle (9pt);
\draw[white,thick] (15pt,-15pt) circle (10pt);
\end{tikzpicture}
\caption{Impact of increasing the depth of the network on the segmentation
performance of very large lesions. The true positive, false negative, and false
positive voxels are highlighted in green, yellow, and red, respectively. The
7-layer CEN, with and without shortcut, is able to segment large lesions much
better than the 3-layer CEN due to the increased size of the receptive field.
This figure is best viewed in color.}
\label{fig:large}
\end{figure}

\begin{figure}
\begin{tikzpicture}[node distance=2.1cm and 0.334\textwidth,
  font=\footnotesize, on grid]
\node[inner sep=0] (image1) {
  \includegraphics[width=\columnwidth]{figures/tmi/p25s35_small_lesions}
};
\node[above=of image1] (l7) {7-layer CEN};
\node[left=of l7] {3-layer CEN};
\node[right=of l7] {7-layer CEN-s};
\begin{scope}[xshift=0.3333\textwidth]
\draw[white,thick] (-12.5pt,-11pt) circle (4pt);
\draw[white,thick] (0pt,4pt) circle (6pt);
\draw[white,thick] (-10pt,20pt) circle (3pt);
\end{scope}
\begin{scope}[xshift=-0.3333\textwidth]
\draw[white,thick] (-12.5pt,-11pt) circle (4pt);
\draw[white,thick] (0pt,4pt) circle (6pt);
\draw[white,thick] (-10pt,20pt) circle (3pt);
\end{scope}
\draw[white,thick] (-12.5pt,-11pt) circle (4pt);
\draw[white,thick] (0pt,4pt) circle (6pt);
\draw[white,thick] (-10pt,20pt) circle (3pt);
\end{tikzpicture}

\caption{Comparison of segmentation performance of different CEN architectures
for small lesions. The white circles indicate lesions that were detected by the
3-layer CEN and the 7-layer CEN, but only with shortcut. Increasing the network
depth decreases the sensitivity to small lesions, but the addition of a
shortcut allows the network to regain this ability, while still being able to
detect large lesions (see Fig.~\ref{fig:large}). This figure is best viewed in
color.}
\label{fig:small}
\end{figure}


\subsection{Comparison for Different Lesion Sizes}

To examine the effect of lesion size on segmentation performance, we stratified
the test set into five groups based on their mean reference lesion size as
summarized in Table~\ref{tab:groups}. A comparison of segmentation accuracy and
lesion detection measures of a 7-layer CEN-s trained on different input
modalities and the best performing competing method LST-LGA for different lesion
sizes is illustrated in Fig.~\ref{fig:sizecomp}. The 7-layer CEN-s outperformed
LST-LGA for all lesions sizes except for very large lesions when trained on T1w
and FLAIR MRIs. The advantage extended to all lesion sizes when the \mbox{CEN-s} was
trained on all four modalities, which could not be done for LST-LGA.
The differences were larger for smaller lesions, which are generally more
challenging to segment for all methods. The differences between the two
approaches were due to a higher sensitivity to lesions as measured by the
LTPR, especially for smaller lesions, while the number of false positives
was approximately the same for all lesion sizes.

\begin{table}[tb]
\caption{Lesion size groups as used for the detailed analysis.}
\label{tab:groups}
\centering
\begin{tabular}{@{}lccc@{}}
\toprule
Group & Mean lesion size [\si{\cubic\milli\metre}] & \#Samples & Lesion
load [\si{\cubic\milli\metre}] \\
\midrule
Very small & $[0,70]$ & 6 & \num{1457+-1492} \\
Small      & $(70,140]$ & 24 & \num{4298+-2683} \\
Medium & $(140,280]$ & 24 & \num{12620+-9991} \\
Large & $(280,500]$ & 14 & \num{13872+-5814} \\
Very large & $> 500$ & 9 & \num{35238+-27531} \\
\bottomrule
\end{tabular}
\end{table}

\begin{figure}
\centering
\includegraphics[width=\textwidth]{figures/tmi/cen_vs_LGA_size}

\caption{Comparison of segmentation accuracy and lesion detection measures of a
7-layer CEN-s trained on different input modalities and the best performing
competing method LST-LGA for different lesion sizes. The 7-layer CEN-s
outperforms LST-LGA for all lesions sizes except for very large lesions when
trained on T1w and FLAIR MRIs, and for all lesion sizes when trained on all four
modalities, due to a higher sensitivity to lesions, while producing
approximately the same number of false positives. Outliers are denoted by black
dots.}

\label{fig:sizecomp}
\end{figure}

\section{Discussion}

The automatic segmentation of MS lesions is a very challenging task due to the
large variability in lesion size, shape, intensity, and location, as well as the
large variability of imaging contrasts produced by different scanners used in
multi-center studies. Most unsupervised methods model lesions as an outlier
class or a separate cluster in a subject-specific model, which makes them
inherently robust to inter-subject and inter-scanner variability. However,
outliers are often not specific to lesions and can also be caused by intensity
inhomogeneities, partial volume, imaging artifacts, and small anatomical
structures such as blood vessels, which leads to the generation of false
positives. On the other hand, supervised methods can learn to discriminate
between lesion and non-lesion tissue, but are more sensitive to the variability
in lesion appearance and different contrasts produced by different scanners. To
overcome those challenges, supervised methods require large data sets that span
the variability in lesion appearance and careful pre-processing to match the
imaging contrast of new images with those of the training set. Library-based
approaches have shown great promise for the segmentation of MS lesions, but do
not scale well to very large data sets due to the large amount of memory
required to store comprehensive sample libraries and the time required to scan
such libraries for matching patches. On the other hand, parametric deep learning
models such as convolutional neural networks scale much better to large training
sets, because the size required to store the model is independent of the
training set size, and the operations required for training and inference are
inherently parallelizable, which allows them to take advantage of very fast
GPU-accelerated computing hardware. Furthermore, the combination of many
nonlinear processing units allows them to learn features that are robust under
large variability, which is crucial for the segmentation of MS lesions.

Convolutional neural networks were originally designed to classify entire images
and designing networks that can segment images remains an important research
topic. Early approaches have formulated the segmentation problem as a patch-wise
classification problem, which allows them to directly use established
classification network architectures for image segmentation.
However, a major limitation of patch-based deep learning approaches is the time
required for training and inference. Fully convolutional networks can perform
the segmentation much more efficiently, but generally lack the precision to
perform voxel-accurate segmentation and cannot handle unbalanced classes.

To overcome these challenges, we have presented a new method for the automatic
segmentation of MS lesions based on deep convolutional encoder networks with
shortcut connections. The joint training of the feature extraction and
prediction pathways allows for the automatic learning of features at different
scales that are tuned for a given combination of image types and segmentation
task. Shortcuts between the two pathways allow high- and low-level features to
be leveraged at the same time for more consistent performance across scales. In
addition, we have proposed a new objective function based on the combination of
sensitivity and specificity, which makes the objective function inherently
robust to unbalanced classes such as MS lesions, which typically comprise less
than \SI{1}{\percent} of all image voxels. We have evaluated our method on two
publicly available data sets and a large data set from an MS clinical trial,
with the results showing that our method performs comparably to the best
state-of-the-art methods, even for relatively small training set sizes. We have
also shown that when a suitably large training set is available, our method is
able to segment MS more accurately than widely-used competing methods such as
EMS, LST-LGA, SLS, and Lesion-TOADS. The substantial gains in accuracy were
mostly due to an increase in lesion sensitivity, especially for small lesions.
Overall, our proposed CEN with shortcut connections performed consistently well
over a wide range of lesion sizes.

Our segmentation framework is very flexible and can be easily extended. One such
extension could be to incorporate prior knowledge about the tissue type of each
non-lesion voxel into the segmentation procedure. The probabilities of each
tissue class could be precomputed by a standard segmentation method, after which
they can be added as an additional channel to the input units of the CEN, which
would allow the CEN to take advantage of intensity information from different
modalities and prior knowledge about each tissue class to carry out the
segmentation. In addition, our method can be applied to other segmentation
tasks. Although we have only focused on the segmentation of MS lesions in this
paper, our method does not make any assumptions specific to MS lesion
segmentation. The features required to carry out the segmentation are solely
learned from training data, which allows our method to be used to segment
different types of pathology or anatomy when a suitable training set is
available.
