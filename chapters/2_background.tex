\chapter{Background: Deep Learning}
\label{sec:background}

Deep learning is a field within machine learning that has been studied since the
early 1980s \citep{fukushima1980}. However, deep learning methods did not gain
in popularity until the late 2000s with the advent of fast general purpose
graphics processors \citep{raina2009}, layerwise pre-training methods
\citep{hinton2006b,hinton2006c}, and large data sets
\citep{deng2009,krizhevsky2012}. Since then, deep learning methods have become
the state-of-the-art in many non-medical \citep{krizhevsky2012,sainath2013} and
medical \citep{ciresan2012,kamnitsas2015} applications. There are many different
algorithms and models that are commonly referred to as deep learning methods,
all of which have two properties in common: 1) the use of multiple layers of
nonlinear processing units for extracting features, and 2) the layers are
organized to form a hierarchy of low-level to high-level features. Representing
data in a feature hierarchy has many advantages for classification and other
applications. To give an example of a feature hierarchy, let us consider the
domain of face images. The lowest layer of the feature hierarchy is composed of
the raw pixel intensities, which are the most basic features of an image.
Multiple pixels can be grouped to form general image features like edges and
corners, which can be further combined to form face parts such as different
variations of noses, eyes, mouths, and ears.
Finally, multiple face parts can be combined to form a variety of face images.
Learning a feature hierarchy facilitates the parameterization of a large feature
space with a small number of values by capturing complex relationships between
feature layers. For example, a feature hierarchy consisting of three
prototypical shapes for mouths, eyes, ears, and noses is able to represent
$\num{3x3x3x3} = 81$ different prototypical faces with only $3+3+3+3=12$
features. Without a hierarchical representation of the data, a model would
require $81$ prototypical face features to span the same face manifold.

In this chapter, we will briefly introduce the supervised and unsupervised deep
learning models that form the basis for the lesion segmentation and manifold
learning methods, which are discussed further in \ref{sec:segmentation} and
\ref{sec:manifold}. We will start with a description of \glspl{dnn}
\citep{farley1954,werbos1974,rumelhart1986} and \glspl{cnn}
\citep{fukushima1980,lecun1989,lecun1998}. In the second part, we will give a
brief overview of \glspl{rbm} \citep{freund1992,hinton2010a}, which are the
building blocks of \glspl{dbn} \citep{hinton2006b}.

% Reduce overlap with introduction)

% RBMs, DBNs, convRBMs, convDBNs for manifold learning and NN and CNN for lesion
% segmentation.

\section[Supervised learning]{Supervised Learning}

% If I put this first, I can argue with inspiration from biological models
% first.

A typical pipeline for classifying images consists of two main steps. In the
first step, predefined or learned features are extracted from the input images,
which are then used to train a separate supervised learning model, such as a
\gls{rf} \citep{breiman2001} or a \gls{svm} \citep{cortes1995},
to perform classification or prediction. Alternatively, classification and
prediction can be performed with a single model that takes the raw input data
and produces the desired output, such as class probabilities. This type of
learning is called end-to-end learning and has shown great potential for medical
image analysis \citep{ciresan2012}. The most popular models for end-to-end
learning are neural networks due to their ability to learn a hierarchical set of
features from raw input data. This allows the learning of features that are
tuned for a given combination of input modalities and classification task, but
is more prone to overfitting than unsupervised feature learning methods,
especially when the amount of labeled data is limited. In this section, we will
start with an introduction to dense neural networks, followed by a concise
overview of convolutional neural networks.

% \begin{itemize}
%   \item Extracted features often fed into a supervised model to perform
%   classification or prediction
%   \item Directly learn the mapping from raw input to final output, without a
%   separated feature extraction step
%   \item Called end-to-end learning
%   \item Multi-layer neural network
%   \item First layers learn to extract features from the input data
%   \item Last layers learn to classify the input
%   \item Minimize the error through backpropagation through all layers yields
%   feature extractor that is tuned to the classification task
%   \item Two types of neural networks are most common: dense neural networks
%   (DNN) and convolutional neural networks (CNN)
% \end{itemize}

\subsection[Dense neural networks]{Dense Neural Networks}
\label{sec:DNN}

A dense neural network (see \ref{fig:dnn}) is a deterministic function that maps
input data to the desired outputs through the successive application of multiple
nonlinear mappings of the following form
\begin{align}
\vect{z}_l &= \vect{W}_l\vect{x}_{l-1} + \vect{b}_l, \\
\vect{x}_l &= f_l(\vect{z}_l),
\end{align}
where $l$ indexes a unit layer, $\vect{x}_0$ denotes a vector containing the
input of the neural network, $\vect{x}_L$ denotes a vector containing the
output, $L$ is the number of computational layers, $f_l$ are transfer functions,
$\vect{W}_l$ are weight matrices, and $\vect{b}_l$ are bias terms.
Popular choices for the transfer function are the sigmoid function $f(x) =
\sigm(x)$ and the rectified linear function $f(x) = \max(0, x)$. The same
transfer function is typically used for all layers except for the output layer.
The choice of the output transfer function depends on the learning task. For
classification, a $1$-of-$n$ encoding of the output class is usually used in
combination with the softmax transfer function defined as
\begin{equation}
\text{softmax}(\vect{a})_i = \frac{\exp(a_i)}{\sum_{j=1}^n \exp(a_j)},
\end{equation}
where $\vect{a}$ denotes an $n$-dimensional output vector.

\begin{figure}
% \missingfigure{Neural network. Show multiple layers and label them with $x_0$,
% $x_l$, and $x_L$. Also label input layer, hidden layers and output layer.}
\centering
\input{tikzfigures/nn}
\caption[Schematic depiction for the calculations in a dense neural
network]{Schematic depiction for the calculations in a dense neural network.
Each hidden and output unit calculates the weighted sum of its inputs followed
by the application of a transfer function.}
\label{fig:dnn}
\end{figure}

Given a training set $\data = \{(\vect{x}_0^{(i)}, \vect{y}^{(i)})\given i
\in [1, N] \}$, a neural network is trained by minimizing the error
between the predicted outputs $\vect{x}_L^{(i)}$ and the given labels
$\vect{y}^{(i)}$
\begin{equation}
\hat{\thetas} = \arg\min_{\thetas} \sum_{i = 1}^N E\Big(\vect{x}_L^{(i)},
\vect{y}^{(i)}\Big),
\end{equation}
where $\thetas$ denotes the trainable parameters of the neural network. Typical
choices for the error function are the \gls{ssd} and the cross-entropy.
The minimization problem can be solved using \gls{sgd}
\citep{rumelhart1986,polyak1992}, which requires the calculation of the gradient
of the error function with respect to the model parameters. The gradient can be
calculated by backpropagation \citep{werbos1974} as follows
\begin{align}
\deltas_L &= \nabla_{\vect{x}_L}E \cdot f_L'(\vect{z}_L), \\
\deltas_l &= (\vect{W}_{l+1}^\text{T}\deltas_{l+1}) \cdot
f_l'(\vect{z}_l) & \text{for $l < L$,}\\
\nabla_{\vect{W}_l}E &= \deltas_l\vect{x}_{l-1}^\text{T}, \\
\nabla_{\vect{b}_l}E &= \deltas_l,
\end{align}
where $\nabla_{\vect{x}_L}E$ denotes the gradient of the error function with
respect to the predicted output and $\cdot$ denotes element-wise multiplication.

\subsection[Convolutional neural networks]{Convolutional Neural Networks}

The structure of CNNs is inspired by the complex arrangement of simple and
complex cells found in the visual cortex \citep{hubel1962,hubel1968}. Simple
cells are only connected to a small sub-region of the previous layer and need to
be tiled to cover the entire visual field. In a CNN (see \ref{fig:cnn}),
simple cells are represented by convolutional layers, which exhibit a similar
mechanism of local connectivity and weight sharing. Complex cells combine the
activations of simple cells to add robustness to small translations. These cells
are represented in the form of pooling layers. After several alternating
convolutional and pooling layers, the activations of the last convolutional
layer are fed into one or more dense layers to carry out the final
classification.

\begin{figure}
\centering
\input{tikzfigures/cnn}
%\includegraphics{figures/Fig-3-3}
\caption[Convolutional neural network with two convolutional layers, one pooling
layer and one dense layer]{Convolutional neural network with two convolutional layers, one
pooling layer and one dense layer. The activations of the last layer are the
output of the network.}
\label{fig:cnn}
\end{figure}

For multimodal $3$D volumes, the neurons of convolutional and pooling layers are
arranged in a $4$D array, where the first three dimensions correspond to the
dimensions of the input volume, and the forth dimension indexes the input
modality or channel. The activations of the output of a convolutional layer are
calculated by
\begin{equation}
x^{(l)}_j = f\Bigg(\sum_{i=1}^C\tilde{w}^{(l)}_{ij}*x^{(l-1)}_i
+ b^{(l)}_j\Bigg),
\end{equation}
where $l$ is the index of a convolutional layer, $x^{(l)}_j$ denotes the $j$th
channel of the output volume, $w^{(l)}_{ij}$ is a $3$D filter kernel connecting
the $i$th channel of the input volume to the $j$th channel of the output volume,
$b_j^{(l)}$ denotes the bias term of the $j$th output channel, and $\tilde{w}$
denotes a flipped version of $w$, i.e. $\tilde{w}(a) = w(-a)$. CNNs can be
trained using stochastic gradient descent, where the gradient can be derived
analogously to dense neural networks and calculated using backpropagation
\citep{lecun1989,lecun1998}.

Different types of operations \citep{scherer2010} have been proposed for the
pooling layers, with the common goal of creating a more compact representation
of the input data. The most commonly used type of pooling is max-pooling.
Therefore, the input to the pooling layer is divided into small blocks and only
the maximum value of each block as passed on to the next layer, which makes the
representation of the input invariant to small translations in addition to
reducing its dimensionality.

A major challenge for gradient-based optimization methods is the choice of an
appropriate learning rate. Classic stochastic gradient descent \citep{lecun1998}
uses a fixed or decaying learning rate, which is the same for all parameters of
the model. However, the partial derivatives of parameters of different layers
can vary substantially in magnitude, which can require different learning rates.
In recent years, there has been an increasing interest in developing methods for
automatically choosing independent learning rates. Most methods (e.g., AdaGrad
by \citealp{duchi2011}; AdaDelta by \citealp{zeiler2012}; RMSprop
by \citealp{dauphin2015}; and Adam by \citealp{kingma2014}) collect different
statistics of the partial derivatives over multiple iterations and use this
information to set an adaptive learning rate for each parameter. This is
especially important for the training of deep networks, where the optimal
learning rates often differ greatly for each layer.

\section[Unsupervised learning]{Unsupervised Learning}

% More statistical learning and finding patterns of similarity.

One of the most important applications of deep learning is to learn patterns of
variability in form of a feature hierarchy from unlabeled images. The key to
learning such a hierarchy is the ability of deep models to be trained layer by
layer, where each layer acts as a nonlinear feature extractor. Various methods
have been proposed for feature extraction from unlabeled images. In this
section, we will first introduce the restricted Boltzmann machines
\citep{freund1992,hinton2010a}, which are the building blocks of the later
described deep belief networks \citep{hinton2006b}.

\subsection[From restricted Boltzmann machines to deep belief networks]{From
Restricted Boltzmann Machines to Deep Belief Networks}

An RBM is a probabilistic graphical model defined by a bipartite graph as shown
in \ref{fig:rbm}. The units of the RBM are divided into two layers, one of
visible units $\vect{v}$ and the other of hidden units $\vect{h}$. There are no
direct connections between units within either layer. An RBM defines the joint
probability of visible and hidden units in terms of the energy $E$
\begin{align}
p(\vect{v}, \vect{h} \given \thetas) &=
\frac{1}{Z(\thetas)}e^{-E(\vect{v}, \vect{h} \given \thetas)}; \\
\intertext{when the visible and hidden units are binary, the energy is defined
as} 
-E(\vect{v}, \vect{h}\given \thetas) &= \sum_{i, j}v_i w_{ij} h_j +
\sum_i b_i v_i + \sum_j c_j h_j \\
&= \vect{v}^\textup{T}\vect{W}\vect{h} + \vect{b}^\textup{T}\vect{v} +
\vect{c}^\textup{T}\vect{h}
\end{align}
where $Z(\thetas)$ is a normalization constant, $\vect{W}$ denotes the weight
matrix that connects the visible units with the hidden units, $\vect{b}$ is a
vector containing the visible bias terms, $\vect{c}$ is a vector containing the
hidden bias terms, and $\thetas = \{\vect{W}, \vect{b}, \vect{c}\}$ are the
trainable parameters of the RBM.

\begin{figure}
\centering
\input{tikzfigures/rbm}
%\includegraphics{figures/Fig-3-1}
\caption[Graph representation of an RBM with 3 hidden and 5 visible units]{Graph
representation of an RBM with 3 hidden and 5 visible units. An RBM models the
joint probability of visible and hidden units. Edges between vertices denote
conditional dependence between the corresponding random variables.}
\label{fig:rbm}
\end{figure}

\subsubsection{Inference}
The hidden units represent patterns of similarity that can be observed in groups
of images. Given a set of model parameters $\thetas$, the features of an image
can be extracted by calculating the expectation of the hidden units. The
posterior distribution of the hidden units given the visible units can be
calculated by
\begin{equation}
\label{eq:dhgivenv}
p(h_j = 1 \given \vect{v}, \thetas) = \sigm(\vect{w}_{\cdot,j}^\text{T}\vect{v}
+ c_j),
\end{equation}
where $\vect{w}_{\cdot,j}$ denotes the $j$th column vector of $\vect{W}$, and
$\sigm(x)$ is the sigmoid function defined as $\sigm(x) = (1+ \exp(-x))^{-1}, x
\in \R$. An RBM is a generative model, which allows for the reconstruction of
an input signal given its features. This is achieved by calculating the expectation
of the visible units given the hidden units. The posterior distribution $p(v_i =
1 \given \vect{h}, \thetas)$ can be calculated by
\begin{equation}
\label{eq:dvgivenh}
p(v_i = 1 \given \vect{h}, \thetas) = \sigm(\vect{w}_{i,
\cdot}^\text{T}\vect{h} + b_i),
\end{equation}
where $\vect{w}_{i,\cdot}$ denotes the $i$th row vector of $\vect{W}$.
Reconstructing the visible units can be used to visualize the learned features.
To visualize the features associated with a particular hidden unit, all other
hidden units are set to zero and the expectation of the visible units is
calculated, which represents the pattern that causes a particular hidden
unit to be activated.

\subsubsection{Training}

% \begin{itemize}
%   \item There are different training methods for RBMs.
%   \item Will focus on constrastive divergence.
%   \item Alternatives are stochastic bla with a reference
%   \item RBMs are trained using maximum likelihood estimation (MLE)
% \end{itemize}

RBMs can be trained by maximizing the likelihood or, more commonly, the
log-likelihood of the training data, $\data = \{\vect{v}_n \given n \in [1, N]
\}$, which is called \gls{mle}. The gradient of the log-likelihood function with
respect to the weights, $\vect{W}$, is given by the mean difference of two
expectations
\begin{equation}
\label{eq:mle}
\nabla_{\vect{W}} \log p(\mathcal{D} \given \thetas) =
\frac{1}{N} \sum_{n = 1}^N
\E[\vect{v}\vect{h}^\text{T} \given \vect{v}_n, \thetas]
-\E[\vect{v}\vect{h}^\text{T} \given \thetas].
\end{equation}
The first expectation can be estimated using a mean field approximation
\begin{align}
\E[\vect{v}\vect{h}^\text{T} \given \vect{v}_n, \thetas] &\approx
\E[\vect{v} \given \vect{v}_n, \thetas]
\E[\vect{h}^\text{T} \given \vect{v}_n, \thetas], \\
&=\vect{v}_n\E[\vect{h}^\text{T} \given \vect{v}_n, \thetas].
\end{align}
The second expectation is typically estimated using a Monte Carlo
approximation
\begin{equation}
\E[\vect{v}\vect{h}^\text{T} \given \thetas] \approx
\frac{1}{S}\sum_{s=1}^{S}\vect{v}_s\vect{h}_s^\text{T},
\end{equation}
where $S$ is the number of generated samples, and $\vect{v}_s$ and $\vect{h}_s$
are samples drawn from $p(\vect{v}\given \thetas)$ and $p(\vect{h}\given
\thetas)$, respectively. Samples from an RBM can be generated efficiently using
block Gibbs sampling, in which the visible and hidden units are initialized
with random values and alternately sampled given the previous state using
\begin{align}
h_j &= \I(y_j < p(h_j = 1 \given \vect{v}, \thetas)) & \text{with $y_j \sim
\text{U}(0,1)$}\\
v_i &= \I(x_i < p(v_i = 1 \given \vect{h}, \thetas)) & \text{with $x_i \sim
\text{U}(0,1)$}
\end{align}
where $z \sim \text{U}(0,1)$ denotes a sample drawn from the uniform
distribution in the interval $[0,1]$ and $\I$~is the indicator function, which
is defined as $1$ if the argument is true and $0$ otherwise. After several
iterations, a sample generated by the Gibbs chain is distributed according to
$p(\vect{vh}\given \thetas)$.

If the Gibbs sampler is initialized at a data point from the training set and
only one Monte Carlo sample is used to approximate the second expectation in
\eqref{eq:mle}, the learning algorithm is called \gls{cd} \citep{hinton2002}.
Alternatively, \gls{pcd} \citep{tieleman2008} uses several separate Gibbs chains
to generate data independent samples from the model, which results in a better
approximation of the gradient of the log-likelihood than CD. To speed up the
training, the data set is usually divided into small subsets called mini-batches
and a gradient step is performed for each mini-batch. To avoid confusion with a
gradient step, the term ``iteration'' is generally avoided and the term
``epoch'' is used instead to indicate a sweep through the entire data set.
Additional tricks to monitor and speed up the training of an RBM can be found in
Hinton's RBM training guide \citep{hinton2010a}. A detailed explanation and
comparison of different training algorithms is given in \ref{sec:training}.

\subsubsection[Deep belief networks]{Deep Belief Networks}

% TODO: Perhaps more explicitly define DBN here and cite (Hinton et al. 2006)?
%
% - A probabilistic generative model 
% - directed edges (vs. DBM)

A single RBM can be regarded as a nonlinear feature extractor. To learn a
hierarchical set of features, multiple RBMs are stacked and trained layer by
layer, where the first RBM is trained on the input data and subsequent RBMs are
trained on the hidden unit activations computed from the previous RBM. The
stacking of RBMs can be repeated to initialize DBNs of any depth.

\subsection[Variants of restricted Boltzmann machines and deep belief
networks]{Variants of Restricted Boltzmann Machines and Deep Belief
Networks}

Many variants of RBMs and DBNs have been proposed to adapt them for different
domains. In this section, we will first introduce the \glspl{cdbn}, which allows
DBNs to be applied to high-resolution images, followed by a discussion of
different unit types, which allow DBNs to be applied to real-valued data like
the intensities of some medical images.

\subsubsection{Convolutional DBNs}

A potential drawback of DBNs is that the learned features are location
dependent. Hence, features that can occur at many different locations in an
image, such as edges and corners, must be relearned for every possible location,
which dramatically increases the number of features required to capture the
content of large images. To increase the translational invariance of the learned
features, Lee et al. introduced the convDBN \citep{lee2009,lee2011}. In a
convDBN, the units of each layer are organized in a multidimensional array that
reflects the arrangement of pixels in the input image. The units of one layer
are only connected to the units of a sub-region of the previous layer, and share
the same weights with all other units of the same layer. This greatly reduces
the number of trainable weights, which reduces the risk of overfitting, reduces
the memory required to store the model parameters, speeds up the training, and
thereby facilitates the application to high-resolution images.

A convDBN consists of alternating convolutional and pooling layers, which are
followed by one or more dense layers. Each convolutional layer of the model can
be trained in a greedy layerwise fashion by treating it as a \gls{crbm}. The
energy of a convRBM is defined as
\begin{align} 
E(\vect{v},\vect{h}) 
&= -\sum_{i=1}^{N_\text{c}} \sum_{j=1}^{N_\text{k}}
\sum_{x,y=1}^{N_\text{h}} \sum_{u,v=1}^{N_\text{w}}
h_{xy}^{(j)}w_{uv}^{(ij)}v_{x+u-1, y+v-1}^{(i)} -
\sum_{i=1}^{N_\text{c}}b_i\!\sum_{x,y = 1}^{N_\text{v}}\!v_{xy}^{(i)} -
\sum_{j=1}^{N_\text{k}}c_j\!\sum_{x,y = 1}^{N_\text{h}}\!h_{xy}^{(j)} \\
&= -\sum_{i=1}^{N_\text{c}} \sum_{j=1}^{N_\text{k}} \vect{h}^{(j)}
\bullet (\tilde{\vect{w}}^{(ij)} * \vect{v}^{(i)}) -
\sum_{i=1}^{N_\text{c}}b_i\sum_{x,y = 1}^{N_\text{v}}\!v_{xy}^{(i)} -
\sum_{j=1}^{N_\text{k}}c_j\sum_{x,y = 1}^{N_\text{h}}\!h_{xy}^{(j)}.
\end{align}
The key terms and notation are defined in \ref{tab:notation}. At the first
layer, the number of channels $N_\text{c}$ is
one when trained on unimodal images, or equal to the number of input
modalities when trained on multimodal images. For subsequent layers,
$N_\text{c}$ is equal to the number of filters of the previous layer.

\begin{table} 
\caption{Key variables and notation. For notational simplicity,
we assume the input images to be square 2D images.}
\label{tab:notation}
\begin{center}
\begin{tabular}{@{}cL{10cm}@{}}
\toprule
Symbol & Description \\
\cmidrule(r){1-1}\cmidrule(l){2-2}
$\vect{v}^{(i)}$ & a 2D array containing the units of the $i$th input channel \\
$\vect{h}^{(j)}$ & a 2D array containing the units of $j$th output channel or
feature map \\
$\vect{w}^{(ij)}$ & a 2D array containing the weights of filter kernels
connecting visible units $\vect{v}^{(i)}$ to hidden units $\vect{h}^{(j)}$ \\
$b_i$ & bias terms of the visible units \\
$c_j$ & bias terms of hidden units \\
$N_\text{c}$ & number of channels of the visible units \\
$N_\text{v}$ & width and height of the image representing the visible units \\
$N_\text{k}$ & number of filters and feature maps \\
$N_\text{h}$ & width and height of a feature map \\
%$N_\text{w}^2$ & number of weights per filter kernel \\
$\bullet$ & element-wise product followed by summation \\
$*$ & valid convolution \\
$\circledast$ & full convolution \\
$\tilde{\vect{w}}^{(ij)}$ & horizontally and vertically flipped version of
$\vect{w}^{(ij)}$, i.e., $\tilde{w}^{(ij)}_{uv} =
w^{(ij)}_{N_\text{w}-u+1,N_\text{w}-v+1}$, where $N_\text{w}$ denotes
the width and height of a filter kernel
\\
\bottomrule
\end{tabular}
\end{center}
\end{table}
The posterior distributions $p(\vect{h} \given \vect{v})$ and $p(\vect{v} \given
\vect{h})$ can be derived from the energy equation and are given by
\begin{align}
p(h_{xy}^{(j)} = 1 \given \vect{v}) &= \sigm\Big(\sum_{i=1}^{N_\text{c}}
(\tilde{\vect{w}}^{(ij)}*\vect{v}^{(i)})_{xy} + c_j\Big), \\ 
p(v_{xy}^{(i)} = 1 \given \vect{h}) &= \sigm\Big(\sum_{j=1}^{N_\text{k}}
(\vect{w}^{(ij)} \circledast \vect{h}^{(j)})_{xy} + b_i\Big).
\end{align}
To train a convRBM on a set of images $\data = \{\vect{v}_n \given n \in
[1,N]\}$, the weights and bias terms can be learned by CD. During each iteration
of the algorithm, the gradient of each parameter is estimated and a gradient
step with a fixed learning rate is applied. The gradient of the filter weights
can be approximated by
\begin{equation}
\Delta \vect{w}^{(ij)} \approx
\frac{1}{N}(\vect{v}^{(i)}_n*\tilde{\vect{h}}^{(j)}_n -
\vect{v}'^{(i)}_n*\tilde{\vect{h}}_n'^{(j)}),
\end{equation}
where $\vect{h}_n^{(j)}$ and $\vect{h}'^{(j)}_n$ are samples drawn from
$p(\vect{h}^{(j)} \given \vect{v}_n)$ and $p(\vect{h}^{(j)} \given
\vect{v}'_n)$, and $\vect{v}'^{(i)}_n = \E[\vect{v}^{(i)} \given \vect{h}_n]$.

\subsubsection{Strided Convolutional Models}

% Why strided! Less memory, faster, no pooling required. Better correspondence
% to dense DBNs. Draw from the journal paper.

Strided convolutions are a type of convolution that shifts the filter kernel as
a sliding window with a step size or stride $s > 1$, stopping at only
$N_\text{v} / s$ positions. Replacing stride-1 convolutions with strided
convolutions, we can define the energy function of a \gls{scrbm} as follows
\begin{equation} 
E(\vect{v},\vect{h}) = 
-\sum_{i=1}^{N_\text{c}}\sum_{j=1}^{N_\text{k}}\sum_{x,y=1}^{N_\text{h}}
\sum_{u,v=1}^{N_\text{w}}
h_{xy}^{(j)} w_{uv}^{(ij)}v_{s(x-1)+u, s(y-1)+v}^{(i)} -
\sum_{i=1}^{N_\text{c}}b_i\!\sum_{x,y = 1}^{N_\text{v}}\!v_{xy}^{(i)} -
\sum_{j=1}^{N_\text{k}}c_j\!\sum_{x,y = 1}^{N_\text{h}}\!h_{xy}^{(j)}
\end{equation}
Strided convolutional RBMs have several advantages over traditional convRBMs.
The use of a strided convolutions reduces the number of hidden units per channel
to $N_\text{h} = N_\text{v} / s$, hence significantly reducing training times
and memory required for storing the hidden units during training. Furthermore,
\glspl{scdbn} do not require pooling layers to reduce the dimensionality,
because dimensionality reduction is already performed within the sconvRBM
layers. Consequently, inference in an sconvDBN is invertible, which allows for
the visualization of detected patterns similar to DBNs. Rules for inference,
sampling, and training of an sconvRBM can be derived analogous to convRBMs.
Furthermore, in \ref{sec:training:strided}, I will show how to convert an
sconvRBM into an equivalent convRBM by reorganizing the hidden units, which
enables efficient training and inference of sconvRBMs.

\subsubsection{Alternative Unit Types}

To model real-valued inputs like the intensities of some medical images, the
binary visible units of an RBM can be replaced with Gaussian visible units, which
leads to the following energy function
\begin{equation} 
-E(\vect{v}, \vect{h}\given \boldsymbol{\theta}) = \sum_{i,
j}\frac{v_i}{\sigma_i} w_{ij} h_j + \sum_i \frac{(v_i - b_i)^2}{2\sigma_i^2} +
\sum_j c_j h_j,
\end{equation}
where the mean of the $i$th visible unit is encoded in the bias term $b_i$,
and its standard deviation is given by $\sigma_i$. Although approaches have been
proposed to learn the standard deviation \citep{cho2011}, the
training data is often simply standardized to have zero mean and unit variance,
which yields the following simplification for the inference of the visible
and hidden units:
\begin{align} 
\label{eq:ghgivenv}
\E[h_j \given \vect{v}, \thetas] &=
\sigm(\vect{w}_{\cdot,j}^\text{T}\vect{v} + c_j),\\
\label{eq:gvgivenh}
\E[v_i \given \vect{h}, \thetas] &= \vect{w}_{i,\cdot}^\text{T}\vect{h} +
b_i.
\end{align}

A binary hidden unit can only encode two states. In order to increase the
expressive power of the hidden units, Nair et al. proposed using \glspl{nrelu}
\citep{nair2010} as the hidden units, and showed that this can improve the
learning performance of RBMs. The signal of an NReLU is the sum of an infinite
number of binary units, all of which having the same weights but different bias
terms. In the special case where the offsets of their bias terms are set to
$-0.5, -1.5, \dotsc$, the sum of their probabilities and therefore the
expectation of an NReLU is extremely close to having a closed form:
\begin{align}
\E[h_j \given \vect{v}, \thetas] &=
\sum_{i=1}^\infty \sigm(\vect{w}_{\cdot,j}^\text{T}\vect{v} + c_j - i + 0.5),\\
&\approx \log(1+\exp(\vect{w}_{\cdot,j}^\text{T}\vect{v} + c_j)).
\end{align}
However, sampling of this type of unit involves the repeated calculation of the
sigmoid function, which can be time-consuming. If a sample is not constrained
to being an integer, a fast approximation can be calculated with
\begin{align} 
h_j &\sim \max(0, \mu_j + \Norm(0, \sigm(\mu_j))), \\
\mu_j &= \vect{w}_{\cdot,j}^\text{T}\vect{v} + c_j,
\end{align}
where $\Norm(0, \sigma^2)$ denotes Gaussian noise.

In this section, we have introduced the most basic deep learning methods, which
form the basis for the segmentation and manifold learning methods explained in
\ref{sec:segmentation} and \ref{sec:manifold}, respectively. The next chapter
details our training algorithm of convDBNs and CNNs in full, along with a
comparison of alternative training methods.
